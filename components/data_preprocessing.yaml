name: Data preprocessing func
description: 'Preprocess data: clean, scale, split.'
inputs:
- {name: dataset_input, type: Dataset}
- {name: test_size, type: Float, default: '0.2', optional: true}
- {name: random_state, type: Integer, default: '42', optional: true}
outputs:
- {name: train_data_output, type: Dataset}
- {name: test_data_output, type: Dataset}
implementation:
  container:
    image: python:3.9
    command:
    - sh
    - -c
    - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
      'pandas==1.5.0' 'numpy==1.23.0' 'scikit-learn==1.2.0' || PIP_DISABLE_PIP_VERSION_CHECK=1
      python3 -m pip install --quiet --no-warn-script-location 'pandas==1.5.0' 'numpy==1.23.0'
      'scikit-learn==1.2.0' --user) && "$0" "$@"
    - sh
    - -ec
    - |
      program_path=$(mktemp)
      printf "%s" "$0" > "$program_path"
      python3 -u "$program_path" "$@"
    - |
      def _make_parent_dirs_and_return_path(file_path: str):
          import os
          os.makedirs(os.path.dirname(file_path), exist_ok=True)
          return file_path

      def data_preprocessing_func(
          dataset_input_path,
          train_data_output_path,
          test_data_output_path,
          test_size = 0.2,
          random_state = 42
      ):
          """Preprocess data: clean, scale, split."""
          import pandas as pd
          from sklearn.model_selection import train_test_split
          from sklearn.preprocessing import StandardScaler

          # Load data
          df = pd.read_csv(dataset_input_path)
          print(f"Loaded dataset: {df.shape}")

          # Clean
          df = df.dropna()

          # Separate features and target
          X = df.iloc[:, :-1].values
          y = df.iloc[:, -1].values

          # Split
          X_train, X_test, y_train, y_test = train_test_split(
              X, y, test_size=test_size, random_state=random_state
          )

          # Scale
          scaler = StandardScaler()
          X_train = scaler.fit_transform(X_train)
          X_test = scaler.transform(X_test)

          # Save train data
          train_df = pd.DataFrame(X_train)
          train_df['target'] = y_train
          train_df.to_csv(train_data_output_path, index=False)

          # Save test data
          test_df = pd.DataFrame(X_test)
          test_df['target'] = y_test
          test_df.to_csv(test_data_output_path, index=False)

          print(f"Train samples: {len(X_train)}, Test samples: {len(X_test)}")
          print(f"Train data saved to {train_data_output_path}")
          print(f"Test data saved to {test_data_output_path}")

      import argparse
      _parser = argparse.ArgumentParser(prog='Data preprocessing func', description='Preprocess data: clean, scale, split.')
      _parser.add_argument("--dataset-input", dest="dataset_input_path", type=str, required=True, default=argparse.SUPPRESS)
      _parser.add_argument("--test-size", dest="test_size", type=float, required=False, default=argparse.SUPPRESS)
      _parser.add_argument("--random-state", dest="random_state", type=int, required=False, default=argparse.SUPPRESS)
      _parser.add_argument("--train-data-output", dest="train_data_output_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
      _parser.add_argument("--test-data-output", dest="test_data_output_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
      _parsed_args = vars(_parser.parse_args())

      _outputs = data_preprocessing_func(**_parsed_args)
    args:
    - --dataset-input
    - {inputPath: dataset_input}
    - if:
        cond: {isPresent: test_size}
        then:
        - --test-size
        - {inputValue: test_size}
    - if:
        cond: {isPresent: random_state}
        then:
        - --random-state
        - {inputValue: random_state}
    - --train-data-output
    - {outputPath: train_data_output}
    - --test-data-output
    - {outputPath: test_data_output}
